{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.read_csv('../data/brown.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "docs = []\n",
    "non_alpha = re.compile(r\"[^ a-z\\-']\")\n",
    "\n",
    "doc_name = None\n",
    "for i in range(b.shape[0]):\n",
    "    if b['filename'][i] != doc_name:\n",
    "        if doc_name is not None:\n",
    "            docs.append(doc)\n",
    "        doc_name = b['filename'][i]\n",
    "        doc = ''\n",
    "        \n",
    "    sent = re.sub(non_alpha, '', b['tokenized_text'][i].lower().replace('--', ' ').replace(\"''\", '')).strip() + \"|| \"\n",
    "    doc += sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "vectors = [np.zeros(300)]\n",
    "\n",
    "idx = 1\n",
    "with open('../data/glove.6B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split()\n",
    "        vocab[tokens[0]] = idx\n",
    "        idx += 1\n",
    "        vector = np.array(tokens[1:], dtype='float')\n",
    "        vectors.append(vector)\n",
    "\n",
    "embedding_matrix = np.vstack(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_width = 13\n",
    "mid_point = window_width // 2\n",
    "final = []\n",
    "non_final = []\n",
    "sample_factor = 1\n",
    "\n",
    "for doc in docs:\n",
    "    tokens = doc.split()\n",
    "    for pos in range(len(tokens) - window_width):\n",
    "        window = tokens[pos:(pos + window_width)]\n",
    "        indices = [vocab.get(word.replace('||', ''), 0) for word in window]\n",
    "        \n",
    "        if window[mid_point].endswith('||'):\n",
    "            yx = [1]\n",
    "            yx.extend(indices)\n",
    "            final.append(yx)\n",
    "        else:\n",
    "            yx = [0]\n",
    "            yx.extend(indices)\n",
    "            non_final.append(yx)\n",
    "            \n",
    "\n",
    "sample_idx = np.random.choice(len(non_final), len(final) * sample_factor)\n",
    "\n",
    "sampled_non_final = [non_final[i] for i in sample_idx]\n",
    "YX = np.vstack([np.vstack(final), np.vstack(sampled_non_final)])\n",
    "np.random.shuffle(YX)\n",
    "Y = YX[:, 0]\n",
    "X = YX[:, 1:]\n",
    "        \n",
    "#Y = \n",
    "#X = np.vstack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "def make_simple_model(word_vectors, window_width, dense_size):\n",
    "    embed = layers.Embedding(word_vectors.shape[0],\n",
    "                             word_vectors.shape[1],\n",
    "                             input_length = window_width,\n",
    "                             weights = [word_vectors],\n",
    "                             trainable = False,\n",
    "                             mask_zero = True)\n",
    "    \n",
    "    word_input = Input(shape=(window_width,), dtype='float32')\n",
    "    vectors = embed(word_input)\n",
    "    \n",
    "    out = layers.Flatten()(vectors)\n",
    "    out = layers.Dropout(rate=0.4)(out)\n",
    "    \n",
    "    out = layers.Dense(dense_size, activation='relu')(out)\n",
    "    out = layers.Dropout(rate=0.2)(out)\n",
    "    out = layers.Dense(dense_size, activation='relu')(out)\n",
    "    out = layers.Dropout(rate=0.2)(out)\n",
    "    \n",
    "    output = layers.Dense(1, activation='sigmoid')(out)\n",
    "    \n",
    "    model = Model(word_input, output)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = make_simple_model(embedding_matrix, window_width, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m1.fit(X, Y, batch_size=64, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([56694, 56694]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "YX1 = np.vstack([non_final, final])\n",
    "np.random.shuffle(YX1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = YX1[:, 0]\n",
    "X1 = YX1[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 801446 samples, validate on 200362 samples\n",
      "801446/801446 [==============================] - 1901s 2ms/sample - loss: 0.1415 - acc: 0.9460 - val_loss: 0.1217 - val_acc: 0.9496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0002a385f8>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.fit(X1, Y1, batch_size=128, epochs=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = Y1[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = m1.predict(X1[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = 0,0,0,0\n",
    "for i, l in enumerate(gold):\n",
    "    s = scores[i]\n",
    "    \n",
    "    if s >= 0.1:\n",
    "        if l == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    else:\n",
    "        if l == 1:\n",
    "            fn += 1\n",
    "        else:\n",
    "            tn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 829 114 7\n"
     ]
    }
   ],
   "source": [
    "print (tp, tn, fp, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12089077412513255"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp/(fp+tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8771929824561403"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp/(tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_self_attention_model(word_vectors, window_width, dense_size):\n",
    "    embed = layers.Embedding(word_vectors.shape[0],\n",
    "                             word_vectors.shape[1],\n",
    "                             input_length = window_width,\n",
    "                             weights = [word_vectors],\n",
    "                             trainable = False,\n",
    "                             mask_zero = True)\n",
    "    \n",
    "    word_input = Input(shape=(window_width,), dtype='float32')\n",
    "    vectors = embed(word_input)\n",
    "    \n",
    "    cnn_layer = layers.Conv1D(\n",
    "        filters=100,\n",
    "        kernel_size=4,\n",
    "        padding='same')\n",
    "    query_value = cnn_layer(vectors)\n",
    "    \n",
    "    self_attended = layers.Attention()([query_value, query_value])\n",
    "    out = layers.Concatenate()([query_value, self_attended])\n",
    "    \n",
    "    out = layers.Dropout(rate=0.4)(out)\n",
    "    \n",
    "    out = layers.Flatten()(out)\n",
    "    out = layers.Dense(dense_size, activation='relu')(out)\n",
    "    out = layers.Dropout(rate=0.2)(out)\n",
    "    out = layers.Dense(dense_size, activation='relu')(out)\n",
    "    out = layers.Dropout(rate=0.2)(out)\n",
    "    \n",
    "    output = layers.Dense(1, activation='sigmoid')(out)\n",
    "    \n",
    "    model = Model(word_input, output)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = make_self_attention_model(embedding_matrix, window_width, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 13)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 13, 300)      120000300   input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 13, 100)      120100      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 13, 100)      0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 13, 200)      0           conv1d_1[0][0]                   \n",
      "                                                                 attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 13, 200)      0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2600)         0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 200)          520200      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 200)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 200)          40200       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 200)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            201         dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 120,681,001\n",
      "Trainable params: 680,701\n",
      "Non-trainable params: 120,000,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90580 samples, validate on 22646 samples\n",
      "Epoch 1/20\n",
      "90580/90580 [==============================] - 444s 5ms/sample - loss: 0.4997 - acc: 0.7528 - val_loss: 0.4489 - val_acc: 0.7838\n",
      "Epoch 2/20\n",
      "90580/90580 [==============================] - 445s 5ms/sample - loss: 0.4404 - acc: 0.7928 - val_loss: 0.4236 - val_acc: 0.8046\n",
      "Epoch 3/20\n",
      "90580/90580 [==============================] - 445s 5ms/sample - loss: 0.4155 - acc: 0.8092 - val_loss: 0.4167 - val_acc: 0.8049\n",
      "Epoch 4/20\n",
      "90580/90580 [==============================] - 444s 5ms/sample - loss: 0.3965 - acc: 0.8193 - val_loss: 0.4123 - val_acc: 0.8123\n",
      "Epoch 5/20\n",
      "90580/90580 [==============================] - 446s 5ms/sample - loss: 0.3787 - acc: 0.8293 - val_loss: 0.4080 - val_acc: 0.8134\n",
      "Epoch 6/20\n",
      "90580/90580 [==============================] - 444s 5ms/sample - loss: 0.3613 - acc: 0.8377 - val_loss: 0.4048 - val_acc: 0.8154\n",
      "Epoch 7/20\n",
      "90580/90580 [==============================] - 444s 5ms/sample - loss: 0.3451 - acc: 0.8463 - val_loss: 0.4113 - val_acc: 0.8145\n",
      "Epoch 8/20\n",
      "90580/90580 [==============================] - 445s 5ms/sample - loss: 0.3310 - acc: 0.8535 - val_loss: 0.4148 - val_acc: 0.8176\n",
      "Epoch 9/20\n",
      "90580/90580 [==============================] - 445s 5ms/sample - loss: 0.3162 - acc: 0.8610 - val_loss: 0.4127 - val_acc: 0.8190\n",
      "Epoch 10/20\n",
      "90580/90580 [==============================] - 446s 5ms/sample - loss: 0.3025 - acc: 0.8675 - val_loss: 0.4155 - val_acc: 0.8183\n",
      "Epoch 11/20\n",
      "90580/90580 [==============================] - 445s 5ms/sample - loss: 0.2899 - acc: 0.8750 - val_loss: 0.4169 - val_acc: 0.8197\n",
      "Epoch 12/20\n",
      "90580/90580 [==============================] - 444s 5ms/sample - loss: 0.2791 - acc: 0.8810 - val_loss: 0.4332 - val_acc: 0.8208\n",
      "Epoch 13/20\n",
      "90580/90580 [==============================] - 445s 5ms/sample - loss: 0.2638 - acc: 0.8872 - val_loss: 0.4474 - val_acc: 0.8170\n",
      "Epoch 14/20\n",
      "90580/90580 [==============================] - 445s 5ms/sample - loss: 0.2573 - acc: 0.8897 - val_loss: 0.4406 - val_acc: 0.8171\n",
      "Epoch 15/20\n",
      "90580/90580 [==============================] - 446s 5ms/sample - loss: 0.2468 - acc: 0.8955 - val_loss: 0.4646 - val_acc: 0.8142\n",
      "Epoch 16/20\n",
      "90580/90580 [==============================] - 445s 5ms/sample - loss: 0.2401 - acc: 0.8987 - val_loss: 0.4564 - val_acc: 0.8163\n",
      "Epoch 17/20\n",
      "90580/90580 [==============================] - 444s 5ms/sample - loss: 0.2315 - acc: 0.9029 - val_loss: 0.4743 - val_acc: 0.8148\n",
      "Epoch 18/20\n",
      "90580/90580 [==============================] - 445s 5ms/sample - loss: 0.2263 - acc: 0.9059 - val_loss: 0.4868 - val_acc: 0.8164\n",
      "Epoch 19/20\n",
      "90580/90580 [==============================] - 446s 5ms/sample - loss: 0.2160 - acc: 0.9108 - val_loss: 0.4756 - val_acc: 0.8141\n",
      "Epoch 20/20\n",
      "90580/90580 [==============================] - 445s 5ms/sample - loss: 0.2119 - acc: 0.9134 - val_loss: 0.4726 - val_acc: 0.8136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f766c015e80>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.fit(X, Y, batch_size=64, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
